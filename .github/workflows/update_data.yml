name: 2. Update Supabase

on:
  # Trigger this workflow ONLY when "Weekly Scrape & Save" finishes successfully
  workflow_run:
    workflows: ["1. Weekly Scrape & Save"]
    types:
      - completed
  workflow_dispatch: # Manual trigger

jobs:
  publish-to-db:
    runs-on: ubuntu-latest
    # Only run if the scraper actually succeeded
    if: ${{ github.event.workflow_run.conclusion == 'success' }}

    steps:
      - name: Checkout Repo (With new data)
        uses: actions/checkout@v3
        with:
          ref: ${{ github.event.workflow_run.head_branch }} 

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'

      - name: Install Dependencies
        run: |
          pip install pandas sqlalchemy psycopg2-binary toml

      - name: Run Publisher
        env:
          DB_PASS: ${{ secrets.DB_PASS }}
          # 1. CHANGE HOST: Use the Direct URL format (db.<project_ref>.supabase.co)
          # Your project ref is 'fvrzvqamufqfjqufxfqp' (extracted from your old username)
          DB_HOST: "db.fvrzvqamufqfjqufxfqp.supabase.co"
          
          # 2. CHANGE PORT: Use 5432 for Direct Connection
          DB_PORT: "5432"
          
          # 3. CHANGE USER: Use just 'postgres' (No suffix needed for Direct)
          DB_USER: "postgres"
          
          DB_NAME: "postgres"
        run: python publish_data.py
