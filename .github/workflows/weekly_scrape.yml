name: Scrape, Publish, and Save

on:
  schedule:
    - cron: '0 22 * * 0' # Weekly run
  workflow_dispatch: # Manual button

jobs:
  run-pipeline:
    runs-on: ubuntu-latest
    
    # ⚠️ PERMISSION TO SAVE FILES
    permissions:
      contents: write

    steps:
      - name: Checkout Code
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'

      - name: Install Chrome
        uses: browser-actions/setup-chrome@v1
        with:
          chrome-version: stable

      - name: Install Dependencies
        run: |
          pip install --upgrade pip
          pip install selenium webdriver-manager pandas sqlalchemy psycopg2-binary toml

      # 1. RUN SCRAPER (Now saves to ./data/)
      - name: Run Scraper
        run: python teduh_scraper_v2.py

      # 2. RUN PUBLISHER (Reads from ./data/)
      - name: Run Publisher
        env:
          DB_PASS: ${{ secrets.DB_PASS }}
          DB_HOST: "aws-0-ap-southeast-1.pooler.supabase.com"
          DB_PORT: "6543"
          DB_NAME: "postgres"
          DB_USER: "postgres.fvrzvqamufqfjqufxfqp"
        run: python publish_data.py

      # 3. SAVE TO GITHUB (Commits the new files)
      - name: Commit and Push Data
        run: |
          git config --global user.name "GitHub Actions Bot"
          git config --global user.email "actions@github.com"
          
          # Add the data folder (matches your repo structure now)
          git add data/
          
          # Commit only if there are changes
          git commit -m "Auto-update: Scraped Data $(date +'%Y-%m-%d')" || echo "No changes to commit"
          git push
