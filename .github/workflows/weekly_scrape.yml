name: Scrape, Publish, and Save

on:
  schedule:
    - cron: '0 22 * * 0' # Runs weekly
  workflow_dispatch: # Button to run manually

jobs:
  run-pipeline:
    runs-on: ubuntu-latest
    
    # ⚠️ IMPORTANT: Give permission to write to the repo
    permissions:
      contents: write

    steps:
      # 1. Checkout Code
      - name: Checkout Code
        uses: actions/checkout@v3

      # 2. Setup Environment
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'

      - name: Install Chrome
        uses: browser-actions/setup-chrome@v1
        with:
          chrome-version: stable

      - name: Install Dependencies
        run: |
          pip install --upgrade pip
          pip install selenium webdriver-manager pandas sqlalchemy psycopg2-binary toml

      # 3. RUN SCRAPER (Creates files on the virtual machine)
      - name: Run Scraper
        run: python teduh_scraper_v2.py

      # 4. RUN PUBLISHER (Uploads those files to Supabase)
      - name: Run Publisher
        env:
          DB_PASS: ${{ secrets.DB_PASS }}
          DB_HOST: "aws-0-ap-southeast-1.pooler.supabase.com"
          DB_PORT: "6543"
          DB_NAME: "postgres"
          DB_USER: "postgres.fvrzvqamufqfjqufxfqp"
        run: python publish_data.py

      # 5. SAVE TO GITHUB (The Missing Piece!)
      - name: Commit and Push Data
        run: |
          git config --global user.name "GitHub Actions Bot"
          git config --global user.email "actions@github.com"
          git add data/
          # Only commit if there are changes
          git commit -m "Auto-update: Scraped Data $(date +'%Y-%m-%d')" || echo "No changes to commit"
          git push
